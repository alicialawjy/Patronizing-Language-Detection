{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5a478b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, RobertaConfig, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "209eb94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of label to numerical label:\n",
      "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n"
     ]
    }
   ],
   "source": [
    "dpm = DontPatronizeMe('.', '.')\n",
    "dpm = DontPatronizeMe('.', 'dontpatronizeme_pcl.tsv')\n",
    "\n",
    "dpm.load_task1()\n",
    "\n",
    "dpm2 = DontPatronizeMe('.', '.')\n",
    "dpm2 = DontPatronizeMe('.', 'dontpatronizeme_categories.tsv')\n",
    "\n",
    "dpm2.load_task2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0043f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dpm.train_task1_df\n",
    "df_cate = dpm2.train_task2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f739909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10464</td>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10465</td>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10466</td>\n",
       "      <td>10467</td>\n",
       "      <td>@@20282330</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ng</td>\n",
       "      <td>\"\"\" She has one huge platform , and informatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10467</td>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\"\"\" Anja Ringgren Loven I ca n't find a word t...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10468</td>\n",
       "      <td>10469</td>\n",
       "      <td>@@16779383</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"\"\" Guinness World Record of 540lbs of 7-layer...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id     keyword country  \\\n",
       "0          1  @@24942188    hopeless      ph   \n",
       "1          2  @@21968160     migrant      gh   \n",
       "2          3  @@16584954   immigrant      ie   \n",
       "3          4   @@7811231    disabled      nz   \n",
       "4          5   @@1494111     refugee      ca   \n",
       "...      ...         ...         ...     ...   \n",
       "10464  10465  @@14297363       women      lk   \n",
       "10465  10466  @@70091353  vulnerable      ph   \n",
       "10466  10467  @@20282330     in-need      ng   \n",
       "10467  10468  @@16753236    hopeless      in   \n",
       "10468  10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                                    text  label orig_label  \n",
       "0      We 're living in times of absolute insanity , ...      0          0  \n",
       "1      In Libya today , there are countless number of...      0          0  \n",
       "2      \"White House press secretary Sean Spicer said ...      0          0  \n",
       "3      Council customers only signs would be displaye...      0          0  \n",
       "4      \"\"\" Just like we received migrants fleeing El ...      0          0  \n",
       "...                                                  ...    ...        ...  \n",
       "10464  \"Sri Lankan norms and culture inhibit women fr...      0          1  \n",
       "10465  He added that the AFP will continue to bank on...      0          0  \n",
       "10466  \"\"\" She has one huge platform , and informatio...      1          3  \n",
       "10467  \"\"\" Anja Ringgren Loven I ca n't find a word t...      1          4  \n",
       "10468  \"\"\" Guinness World Record of 540lbs of 7-layer...      1          3  \n",
       "\n",
       "[10469 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad2305",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7cd9bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "new= df['text'].str.split()\n",
    "new=new.values.tolist()\n",
    "corpus=[word for i in new for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "my_dict=defaultdict(int)\n",
    "for word in corpus:\n",
    "    my_dict[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "dcde2fea",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-73-93294def8954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# get values in the same order as keys, and parse percentage values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mvals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkeys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbarplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mbarplot\u001b[0;34m(x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge, ax, **kwargs)\u001b[0m\n\u001b[1;32m   3147\u001b[0m                           \u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3148\u001b[0m                           \u001b[0morient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3149\u001b[0;31m                           errcolor, errwidth, capsize, dodge)\n\u001b[0m\u001b[1;32m   3150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3151\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0max\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, hue, data, order, hue_order, estimator, ci, n_boot, units, orient, color, palette, saturation, errcolor, errwidth, capsize, dodge)\u001b[0m\n\u001b[1;32m   1605\u001b[0m         \u001b[0;34m\"\"\"Initialize the plotter.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1606\u001b[0m         self.establish_variables(x, y, hue, data, orient,\n\u001b[0;32m-> 1607\u001b[0;31m                                  order, hue_order, units)\n\u001b[0m\u001b[1;32m   1608\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestablish_colors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpalette\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msaturation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1609\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimate_statistic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mci\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_boot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36mestablish_variables\u001b[0;34m(self, x, y, hue, data, orient, order, hue_order, units)\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0;31m# Group the numeric data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 plot_data, value_label = self._group_longform(vals, groups,\n\u001b[0;32m--> 207\u001b[0;31m                                                               group_names)\n\u001b[0m\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;31m# Now handle the hue levels for nested ordering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/seaborn/categorical.py\u001b[0m in \u001b[0;36m_group_longform\u001b[0;34m(self, vals, grouper, order)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m                 \u001b[0mg_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrouped_vals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_group\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mg_vals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/groupby.py\u001b[0m in \u001b[0;36mget_group\u001b[0;34m(self, name, obj)\u001b[0m\n\u001b[1;32m    680\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   4442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4443\u001b[0m         result = self._constructor(\n\u001b[0;32m-> 4444\u001b[0;31m             \u001b[0mnew_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4445\u001b[0m         ).__finalize__(self)\n\u001b[1;32m   4446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;31m# data is an ndarray, index is defined\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1515\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1516\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1518\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3242\u001b[0m     \u001b[0;31m# Ensure that we don't allow PandasArray / PandasDtype in internals.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3243\u001b[0m     \u001b[0;31m# For now, blocks should be backed by ndarrays when possible.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3244\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCPandasArray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3245\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3246\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "keys = list(my_dict.keys())\n",
    "# get values in the same order as keys, and parse percentage values\n",
    "vals = [int(my_dict[k]) for k in keys]\n",
    "sns.barplot(x=keys, y=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830deec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'living',\n",
       " 'in',\n",
       " 'times',\n",
       " 'of',\n",
       " 'absolute',\n",
       " 'insanity',\n",
       " ',',\n",
       " 'as',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'pretty',\n",
       " 'sure',\n",
       " 'most',\n",
       " 'people',\n",
       " 'are',\n",
       " 'aware',\n",
       " '.',\n",
       " 'For',\n",
       " 'a',\n",
       " 'while',\n",
       " 'waking',\n",
       " 'up',\n",
       " 'every',\n",
       " 'day',\n",
       " 'to',\n",
       " 'check',\n",
       " 'the',\n",
       " 'news',\n",
       " 'seemed',\n",
       " 'carry',\n",
       " 'with',\n",
       " 'it',\n",
       " 'same',\n",
       " 'feeling',\n",
       " 'panic',\n",
       " 'and',\n",
       " 'dread',\n",
       " 'that',\n",
       " 'action',\n",
       " 'heroes',\n",
       " 'probably',\n",
       " 'face',\n",
       " 'when',\n",
       " 'they',\n",
       " 'trying',\n",
       " 'decide',\n",
       " 'whether',\n",
       " 'cut',\n",
       " 'blue',\n",
       " 'or',\n",
       " 'green',\n",
       " 'wire',\n",
       " 'on',\n",
       " 'ticking',\n",
       " 'bomb',\n",
       " '--',\n",
       " 'except',\n",
       " \"'s\",\n",
       " 'instructions',\n",
       " 'long',\n",
       " 'ago',\n",
       " 'burned',\n",
       " 'fire',\n",
       " 'imminent',\n",
       " 'catastrophe',\n",
       " 'seems',\n",
       " 'likeliest',\n",
       " 'outcome',\n",
       " 'It',\n",
       " 'hard',\n",
       " 'stay',\n",
       " 'on-edge',\n",
       " 'for',\n",
       " 'though',\n",
       " 'so',\n",
       " 'natural',\n",
       " 'become',\n",
       " 'inured',\n",
       " 'this',\n",
       " 'constant',\n",
       " 'chaos',\n",
       " 'slump',\n",
       " 'into',\n",
       " 'malaise',\n",
       " 'hopelessness',\n",
       " 'pessimism',\n",
       " 'In',\n",
       " 'Libya',\n",
       " 'today',\n",
       " 'there',\n",
       " 'countless',\n",
       " 'number',\n",
       " 'Ghanaian',\n",
       " 'Nigerian',\n",
       " 'immigrants',\n",
       " 'These',\n",
       " 'two',\n",
       " 'countries',\n",
       " 'key',\n",
       " 'macroeconomic',\n",
       " 'challenges',\n",
       " 'including',\n",
       " 'unemployment',\n",
       " 'Let',\n",
       " 'tackle',\n",
       " 'issue',\n",
       " 'from',\n",
       " 'root',\n",
       " 'not',\n",
       " 'fruit',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '\"White',\n",
       " 'House',\n",
       " 'press',\n",
       " 'secretary',\n",
       " 'Sean',\n",
       " 'Spicer',\n",
       " 'said',\n",
       " 'focus',\n",
       " 'immigration',\n",
       " 'actions',\n",
       " 'would',\n",
       " 'be',\n",
       " 'illegal',\n",
       " '\"\"',\n",
       " 'who',\n",
       " 'have',\n",
       " 'also',\n",
       " 'otherwise',\n",
       " 'violated',\n",
       " 'our',\n",
       " 'laws',\n",
       " '.\"',\n",
       " 'Council',\n",
       " 'customers',\n",
       " 'only',\n",
       " 'signs',\n",
       " 'displayed',\n",
       " 'Two',\n",
       " 'spaces',\n",
       " 'reserved',\n",
       " 'disabled',\n",
       " 'persons',\n",
       " 'five',\n",
       " 'P30',\n",
       " 'eight',\n",
       " 'P60',\n",
       " 'ones',\n",
       " '\"\"\"',\n",
       " 'Just',\n",
       " 'like',\n",
       " 'we',\n",
       " 'received',\n",
       " 'migrants',\n",
       " 'fleeing',\n",
       " 'El',\n",
       " 'Salvador',\n",
       " 'Guatemala',\n",
       " \"'\",\n",
       " '80s',\n",
       " '90s',\n",
       " 'just',\n",
       " \"'ve\",\n",
       " 'seen',\n",
       " 'thousands',\n",
       " 'economic',\n",
       " 'refugees',\n",
       " 'started',\n",
       " 'seeing',\n",
       " 'Mexicans',\n",
       " 'violence',\n",
       " 'Ju',\n",
       " '?',\n",
       " 'rez',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'Mexico',\n",
       " 'says',\n",
       " 'shelter',\n",
       " 'director',\n",
       " 'Ruben',\n",
       " 'Garcia',\n",
       " 'To',\n",
       " 'bring',\n",
       " 'down',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'levels',\n",
       " 'insulin',\n",
       " 'needs',\n",
       " 'taken',\n",
       " 'If',\n",
       " 'type',\n",
       " 'requires',\n",
       " 'during',\n",
       " 'meal',\n",
       " 'time',\n",
       " 'will',\n",
       " 'take',\n",
       " 'correct',\n",
       " 'doses',\n",
       " 'order',\n",
       " 'lower',\n",
       " 'your',\n",
       " 'glucose',\n",
       " 'Decision',\n",
       " 'inject',\n",
       " 'how',\n",
       " 'many',\n",
       " 'help',\n",
       " 'health',\n",
       " 'care',\n",
       " 'professional',\n",
       " 'The',\n",
       " 'European',\n",
       " 'Union',\n",
       " 'is',\n",
       " 'making',\n",
       " 'an',\n",
       " 'historic',\n",
       " 'mistake',\n",
       " 'its',\n",
       " 'haste',\n",
       " 'conclude',\n",
       " 'refugee',\n",
       " 'deal',\n",
       " 'Turkey',\n",
       " 'overlooking',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'violations',\n",
       " 'risk',\n",
       " 'plunging',\n",
       " 'bloc',\n",
       " 'largest',\n",
       " 'membership',\n",
       " 'candidate',\n",
       " 'civil',\n",
       " 'war',\n",
       " 'Selahattin',\n",
       " 'Demirtas',\n",
       " 'leader',\n",
       " 'nation',\n",
       " 'prominent',\n",
       " 'pro-Kurdish',\n",
       " 'party',\n",
       " 'They',\n",
       " 'either',\n",
       " 'hopeless',\n",
       " 'being',\n",
       " 'beaten',\n",
       " 'by',\n",
       " '10-year-old',\n",
       " 'if',\n",
       " 'beat',\n",
       " 'him',\n",
       " 'big',\n",
       " 'bully',\n",
       " 'That',\n",
       " 'joke',\n",
       " 'reality',\n",
       " 'he',\n",
       " 'got',\n",
       " 'great',\n",
       " 'game',\n",
       " 'NUEVA',\n",
       " 'ERA',\n",
       " 'Ilocos',\n",
       " 'Norte',\n",
       " '-',\n",
       " 'No',\n",
       " 'family',\n",
       " 'shall',\n",
       " 'homeless',\n",
       " 'under',\n",
       " 'watch',\n",
       " 'municipal',\n",
       " 'government',\n",
       " 'here',\n",
       " 'town',\n",
       " 'Mayor',\n",
       " 'Aldrin',\n",
       " 'Garvida',\n",
       " 'His',\n",
       " 'spokesman',\n",
       " 'Kremlin',\n",
       " 'needed',\n",
       " 'more',\n",
       " 'information',\n",
       " 'about',\n",
       " 'rebels',\n",
       " 'decision',\n",
       " 'He',\n",
       " 'rebel',\n",
       " 'statement',\n",
       " 'came',\n",
       " 'after',\n",
       " 'Western-backed',\n",
       " 'Kiev',\n",
       " 'had',\n",
       " 'declared',\n",
       " 'military',\n",
       " 'operation',\n",
       " 'implying',\n",
       " 'Ukraine',\n",
       " 'was',\n",
       " 'blame',\n",
       " 'refusal',\n",
       " 'heed',\n",
       " 'Putin',\n",
       " 'A',\n",
       " 'federal',\n",
       " 'appeals',\n",
       " 'court',\n",
       " 'Tuesday',\n",
       " 'cleared',\n",
       " 'way',\n",
       " '17-year-old',\n",
       " 'immigrant',\n",
       " 'held',\n",
       " 'custody',\n",
       " 'Texas',\n",
       " 'obtain',\n",
       " 'abortion',\n",
       " 'full',\n",
       " 'US',\n",
       " 'Court',\n",
       " 'Appeals',\n",
       " 'District',\n",
       " 'Columbia',\n",
       " 'Circuit',\n",
       " 'ruled',\n",
       " '6-3',\n",
       " 'favor',\n",
       " 'teen',\n",
       " 'overturned',\n",
       " 'ruling',\n",
       " 'three-judge',\n",
       " 'panel',\n",
       " 'at',\n",
       " 'least',\n",
       " 'temporarily',\n",
       " 'blocked',\n",
       " 'her',\n",
       " 'getting',\n",
       " 'Trump',\n",
       " 'administration',\n",
       " 'could',\n",
       " 'still',\n",
       " 'appeal',\n",
       " 'Supreme',\n",
       " 'find',\n",
       " 'ourselves',\n",
       " 'situation',\n",
       " 'because',\n",
       " 'longer',\n",
       " 'local',\n",
       " 'factors',\n",
       " 'requiring',\n",
       " 'nursing',\n",
       " 'their',\n",
       " 'home',\n",
       " 'increasing',\n",
       " 'coupled',\n",
       " 'increased',\n",
       " 'demands',\n",
       " 'council-funded',\n",
       " 'vulnerable',\n",
       " 'within',\n",
       " 'own',\n",
       " 'homes',\n",
       " 'Ghostbusters',\n",
       " 'resurrection',\n",
       " '1984',\n",
       " 'hit',\n",
       " 'film',\n",
       " 'but',\n",
       " 'leads',\n",
       " 'women',\n",
       " 'men',\n",
       " 'Government',\n",
       " 'commitment',\n",
       " 'protect',\n",
       " 'groups',\n",
       " 'Najib',\n",
       " '2017',\n",
       " 'Budget',\n",
       " 'proposed',\n",
       " 'financial',\n",
       " 'assistance',\n",
       " 'poor',\n",
       " 'families',\n",
       " 'General',\n",
       " 'Assistance',\n",
       " 'RM300',\n",
       " 'per',\n",
       " 'month',\n",
       " 'Children',\n",
       " 'RM450',\n",
       " 'When',\n",
       " 'my',\n",
       " 'dad',\n",
       " 'died',\n",
       " 'unexpectedly',\n",
       " '2014',\n",
       " 'putting',\n",
       " 'finishing',\n",
       " 'touches',\n",
       " 'his',\n",
       " 'forthcoming',\n",
       " 'book',\n",
       " 'Climate',\n",
       " 'Change',\n",
       " 'Health',\n",
       " 'Nations',\n",
       " 'examines',\n",
       " 'historical',\n",
       " 'records',\n",
       " 'finds',\n",
       " 'very',\n",
       " 'sensitive',\n",
       " 'changes',\n",
       " 'climate',\n",
       " 'And',\n",
       " 'drawing',\n",
       " 'history',\n",
       " 'warns',\n",
       " 'impacts',\n",
       " 'climatic',\n",
       " 'change',\n",
       " 'such',\n",
       " 'environmental',\n",
       " 'disaster',\n",
       " 'temperatures',\n",
       " 'food',\n",
       " 'insecurity',\n",
       " 'amplify',\n",
       " 'infectious',\n",
       " 'diseases',\n",
       " 'under-nutrition',\n",
       " 'heat-related',\n",
       " 'deaths',\n",
       " 'especially',\n",
       " 'among',\n",
       " 'populations',\n",
       " 'children',\n",
       " 'forcibly',\n",
       " 'displaced',\n",
       " 'Apart',\n",
       " 'Pakistan',\n",
       " 'hosts',\n",
       " 'England',\n",
       " 'Bangladesh',\n",
       " 'cricket',\n",
       " 'team',\n",
       " 'participate',\n",
       " 'tournament',\n",
       " '<h>',\n",
       " 'Shahid',\n",
       " 'Afridi',\n",
       " 'bags',\n",
       " '11',\n",
       " 'Man',\n",
       " 'Match',\n",
       " 'awards',\n",
       " 'T20Is',\n",
       " '(',\n",
       " 'Most',\n",
       " 'any',\n",
       " 'player',\n",
       " ')',\n",
       " 'Ab',\n",
       " 'De',\n",
       " 'villiers',\n",
       " 'bagged',\n",
       " 'pair',\n",
       " 'first',\n",
       " 'Test',\n",
       " 'career',\n",
       " 'Super',\n",
       " 'Gossip',\n",
       " 'those',\n",
       " 'few',\n",
       " 'seconds',\n",
       " 'humanity',\n",
       " 'free',\n",
       " 'shackles',\n",
       " 'one',\n",
       " 'glimpses',\n",
       " 'hope',\n",
       " 'amid',\n",
       " 'Peace',\n",
       " 'returned',\n",
       " 'Assam',\n",
       " 'both',\n",
       " 'Hindus',\n",
       " 'Muslims',\n",
       " 'agreed',\n",
       " 'accept',\n",
       " '1971',\n",
       " 'cut-off',\n",
       " 'year',\n",
       " 'citizenship',\n",
       " 'Today',\n",
       " 'wants',\n",
       " 'push',\n",
       " 'state',\n",
       " 'towards',\n",
       " 'unrest',\n",
       " 'convention',\n",
       " 'organized',\n",
       " 'Krishak',\n",
       " 'Mukti',\n",
       " 'Sangram',\n",
       " 'Samiti',\n",
       " 'urged',\n",
       " 'arrived',\n",
       " 'before',\n",
       " 'launch',\n",
       " 'united',\n",
       " 'movement',\n",
       " 'scuttle',\n",
       " 'Centre',\n",
       " 'plan',\n",
       " 'regularize',\n",
       " 'entry',\n",
       " 'Hindu',\n",
       " 'She',\n",
       " 'since',\n",
       " 'total',\n",
       " '8000',\n",
       " 'intellectually',\n",
       " 'Ghana',\n",
       " 'been',\n",
       " 'offered',\n",
       " 'special',\n",
       " 'training',\n",
       " 'enhance',\n",
       " 'integration',\n",
       " 'society',\n",
       " '\"In',\n",
       " 'short',\n",
       " 'reasons',\n",
       " 'behind',\n",
       " 'look',\n",
       " 'reasonable',\n",
       " 'problem',\n",
       " 'however',\n",
       " 'has',\n",
       " 'implementation',\n",
       " 'side',\n",
       " 'much',\n",
       " 'talk',\n",
       " 'success',\n",
       " 'grounds',\n",
       " 'all',\n",
       " 'left',\n",
       " 'voluntarily',\n",
       " 'origin',\n",
       " 'others',\n",
       " 'repatriated',\n",
       " 'leaving',\n",
       " 'country',\n",
       " 'given',\n",
       " 'circumstances',\n",
       " 'optional',\n",
       " 'Sisi',\n",
       " 'lawyer',\n",
       " 'Grisel',\n",
       " 'Ybarra',\n",
       " 'Cuban',\n",
       " 'community',\n",
       " 'edge',\n",
       " 'ongoing',\n",
       " 'negotiations',\n",
       " 'between',\n",
       " 'Washington',\n",
       " 'Havana',\n",
       " 'uncertainty',\n",
       " 'what',\n",
       " 'renewed',\n",
       " 'relations',\n",
       " 'mean',\n",
       " 'Might',\n",
       " 'pain',\n",
       " 'Dr.',\n",
       " 'Francis',\n",
       " 'Javier',\n",
       " 'until',\n",
       " 'recently',\n",
       " 'headed',\n",
       " 'Pain',\n",
       " 'Management',\n",
       " 'Center',\n",
       " 'St.',\n",
       " 'Luke',\n",
       " 'Medical',\n",
       " 'looking',\n",
       " 'genetics',\n",
       " 'comparing',\n",
       " 'Filipinos',\n",
       " 'mixed',\n",
       " 'ancestries',\n",
       " 'see',\n",
       " 'differences',\n",
       " 'medicines',\n",
       " 'broken',\n",
       " 'body',\n",
       " 'highlights',\n",
       " 'different',\n",
       " 'communities',\n",
       " 'respond',\n",
       " 'homelessness',\n",
       " 'ways',\n",
       " 'Kneebone',\n",
       " 'regional',\n",
       " 'brands',\n",
       " 'far',\n",
       " 'lag',\n",
       " 'global',\n",
       " 'international',\n",
       " 'Chinese',\n",
       " 'handset',\n",
       " 'players',\n",
       " '4G',\n",
       " 'looked',\n",
       " 'failing',\n",
       " 'jump',\n",
       " 'generation',\n",
       " 'successfully',\n",
       " 'lose',\n",
       " 'place',\n",
       " 'incidents',\n",
       " 'deepened',\n",
       " 'public',\n",
       " 'doubts',\n",
       " 'Chancellor',\n",
       " 'Angela',\n",
       " 'Merkel',\n",
       " 'open',\n",
       " 'Germany',\n",
       " 'doors',\n",
       " 'whose',\n",
       " 'numbers',\n",
       " 'reached',\n",
       " '1.1',\n",
       " 'million',\n",
       " 'last',\n",
       " 'Some',\n",
       " 'say',\n",
       " 'review',\n",
       " 'legislation',\n",
       " 'sex',\n",
       " 'work',\n",
       " 'helps',\n",
       " 'involved',\n",
       " 'services',\n",
       " 'provided',\n",
       " 'these',\n",
       " 'think',\n",
       " 'know',\n",
       " 'led',\n",
       " 'me',\n",
       " 'detained',\n",
       " 'hours',\n",
       " 'opposed',\n",
       " 'another',\n",
       " 'Sudanese',\n",
       " 'person',\n",
       " '30',\n",
       " 'seventies',\n",
       " 'So',\n",
       " 'aspect',\n",
       " 'On',\n",
       " 'flip',\n",
       " 'went',\n",
       " 'Terminal',\n",
       " '4',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'background',\n",
       " 'feel',\n",
       " 'guess',\n",
       " 'point',\n",
       " 'actually',\n",
       " 'want',\n",
       " 'make',\n",
       " 'reflection',\n",
       " 'larger',\n",
       " 'trend',\n",
       " 'criminalise',\n",
       " 'black',\n",
       " 'Muslim',\n",
       " 'really',\n",
       " 'concerned',\n",
       " 'do',\n",
       " 'Somalis',\n",
       " 'African',\n",
       " 'descent',\n",
       " 'going',\n",
       " 'affected',\n",
       " 'treated',\n",
       " 'differently',\n",
       " 'frankly',\n",
       " 'This',\n",
       " '4th',\n",
       " 'July',\n",
       " 'falls',\n",
       " 'Monday',\n",
       " 'mere',\n",
       " '24',\n",
       " 'kick',\n",
       " 'back',\n",
       " 'and/or',\n",
       " 'holiday',\n",
       " 'grind',\n",
       " 'With',\n",
       " 'crunch',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'might',\n",
       " 'seem',\n",
       " 'never',\n",
       " 'fear',\n",
       " ':',\n",
       " 'Lifehacker',\n",
       " '!',\n",
       " 'Here',\n",
       " 'some',\n",
       " 'tips',\n",
       " 'dive',\n",
       " 'headfirst',\n",
       " 'come',\n",
       " 'out',\n",
       " 'intact',\n",
       " 'Farooqui',\n",
       " 'were',\n",
       " 'demanding',\n",
       " 'fast-track',\n",
       " 'courts',\n",
       " 'rape',\n",
       " 'crimes',\n",
       " 'against',\n",
       " 'right',\n",
       " 'Why',\n",
       " 'pay',\n",
       " '50,000',\n",
       " 'riel',\n",
       " 'acknowledge',\n",
       " 'Cambodian',\n",
       " 'citizen',\n",
       " 'early',\n",
       " 'thing',\n",
       " 'Mr.',\n",
       " 'Yang',\n",
       " 'Ros',\n",
       " 'asked',\n",
       " 'Nearly',\n",
       " '110',\n",
       " 'made',\n",
       " 'blaze',\n",
       " 'housed',\n",
       " 'hotels',\n",
       " 'west',\n",
       " 'London',\n",
       " 'Churches',\n",
       " 'centres',\n",
       " 'providing',\n",
       " 'meals',\n",
       " 'support',\n",
       " 'donations',\n",
       " 'clothing',\n",
       " 'toys',\n",
       " 'household',\n",
       " 'supplies',\n",
       " 'flooding',\n",
       " '\"Cardinal',\n",
       " 'Tagle',\n",
       " 'visited',\n",
       " 'settlements',\n",
       " 'part',\n",
       " 'role',\n",
       " 'Caritas',\n",
       " 'president',\n",
       " 'asks',\n",
       " 'express',\n",
       " 'reservations',\n",
       " 'receiving',\n",
       " 'Have',\n",
       " 'ever',\n",
       " 'talked',\n",
       " 'real',\n",
       " 'heard',\n",
       " 'stories',\n",
       " 'Arshad',\n",
       " 'besides',\n",
       " 'learning',\n",
       " 'new',\n",
       " 'aspects',\n",
       " 'sports',\n",
       " 'leadership',\n",
       " 'learnt',\n",
       " 'fast-developing',\n",
       " 'nations',\n",
       " 'using',\n",
       " 'tool',\n",
       " 'development',\n",
       " 'effort',\n",
       " 'underprivileged',\n",
       " 'stage',\n",
       " 'Fast',\n",
       " 'employee',\n",
       " 'fed',\n",
       " 'man',\n",
       " 'becomes',\n",
       " 'internet',\n",
       " 'sensation',\n",
       " '\"The',\n",
       " '28',\n",
       " 'leaders',\n",
       " 'ask',\n",
       " 'Davutoglu',\n",
       " 'large-scale',\n",
       " 'deportations',\n",
       " 'Greece',\n",
       " 'main',\n",
       " 'Europe',\n",
       " 'implement',\n",
       " 'November',\n",
       " 'slow',\n",
       " 'flow',\n",
       " 'foundation',\n",
       " 'willing',\n",
       " 'raise',\n",
       " 'fund',\n",
       " 'victims',\n",
       " 'click',\n",
       " 'link',\n",
       " 'below',\n",
       " 'donation',\n",
       " 'broke',\n",
       " 'labor',\n",
       " 'camp',\n",
       " 'Al',\n",
       " 'Sailiya',\n",
       " '8th',\n",
       " 'May',\n",
       " 'approximately',\n",
       " '300',\n",
       " 'Sri',\n",
       " 'Lankans',\n",
       " 'employed',\n",
       " 'Janitorial',\n",
       " 'Service',\n",
       " 'Company',\n",
       " 'Qatar',\n",
       " 'temporary',\n",
       " 'lodging',\n",
       " 'Expatriates',\n",
       " 'report',\n",
       " 'arrested',\n",
       " 'authorities',\n",
       " 'Rizvi',\n",
       " 'There',\n",
       " 'no',\n",
       " 'joy',\n",
       " 'Eid',\n",
       " 'citizens',\n",
       " 'becoming',\n",
       " 'FLOODTwo',\n",
       " 'weeks',\n",
       " 'farms',\n",
       " 'submerged',\n",
       " 'water',\n",
       " 'rendering',\n",
       " 'Garissa',\n",
       " 'Tana',\n",
       " 'River',\n",
       " '800,000',\n",
       " 'undocumented',\n",
       " 'expected',\n",
       " 'benefit',\n",
       " 'policy',\n",
       " 'them',\n",
       " 'receive',\n",
       " 'deferred',\n",
       " 'deportation',\n",
       " 'Undocumented',\n",
       " 'U.S.',\n",
       " 'age',\n",
       " '16',\n",
       " 'lived',\n",
       " 'than',\n",
       " 'years',\n",
       " 'apply',\n",
       " 'relief',\n",
       " 'once',\n",
       " 'Justin',\n",
       " 'Bour',\n",
       " 'off',\n",
       " 'ninth',\n",
       " '22nd',\n",
       " 'homer',\n",
       " 'six-week',\n",
       " 'stint',\n",
       " 'list',\n",
       " 'Ramos',\n",
       " 'then',\n",
       " 'gave',\n",
       " 'four',\n",
       " 'singles',\n",
       " 'two-out',\n",
       " 'RBI',\n",
       " 'hits',\n",
       " 'pinch-hitter',\n",
       " 'A.J.',\n",
       " 'Ellis',\n",
       " 'Ichiro',\n",
       " 'Suzuki',\n",
       " 'Vanessa',\n",
       " 'feelings',\n",
       " 'final',\n",
       " 'days',\n",
       " 'which',\n",
       " 'cry',\n",
       " 'yielded',\n",
       " 'response',\n",
       " 'resident',\n",
       " 'magistrate',\n",
       " 'placed',\n",
       " 'State',\n",
       " 'Her',\n",
       " 'letter',\n",
       " 'conveyed',\n",
       " 'official',\n",
       " 'investigator',\n",
       " 'Office',\n",
       " 'Advocate',\n",
       " 'able',\n",
       " 'access',\n",
       " 'Gwauya',\n",
       " 'mother',\n",
       " 'Sithela',\n",
       " 'told',\n",
       " 'Thomson',\n",
       " 'Reuters',\n",
       " 'Foundation',\n",
       " 'Comrade',\n",
       " 'David',\n",
       " 'Kerigbo',\n",
       " 'Ugondo',\n",
       " 'born',\n",
       " 'Late',\n",
       " 'Pa',\n",
       " 'Akerigbo',\n",
       " 'Adikpo',\n",
       " 'wife',\n",
       " 'Mrs.',\n",
       " 'Pam',\n",
       " '25th',\n",
       " 'October',\n",
       " '1950',\n",
       " 'Achagh',\n",
       " 'Mbaduku',\n",
       " 'Vandeikya',\n",
       " 'Local',\n",
       " 'Benue',\n",
       " '67th',\n",
       " 'birthday',\n",
       " 'well',\n",
       " 'secured',\n",
       " 'Nigeria',\n",
       " 'birth',\n",
       " 'nurtured',\n",
       " '27',\n",
       " 'meritorious',\n",
       " 'service',\n",
       " 'industry',\n",
       " 'unionism',\n",
       " 'tragic',\n",
       " 'unsecured',\n",
       " 'day-light',\n",
       " 'gunshots',\n",
       " 'Sunday',\n",
       " 'September',\n",
       " '10',\n",
       " 'criminal',\n",
       " 'armed',\n",
       " 'robbers',\n",
       " 'attacked',\n",
       " 'around',\n",
       " 'Birnin',\n",
       " 'Gwari',\n",
       " 'Kaduna',\n",
       " 'God',\n",
       " 'grant',\n",
       " ...]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40741202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6fd19",
   "metadata": {},
   "source": [
    "### Sample text to visualise tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "068ce4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8667, 106, 146, 1567, 1115, 1128, 1132, 1177, 2869, 119]\n",
      "10\n",
      "['Hello', '!', 'I', 'love', 'that', 'you', 'are', 'so', 'poor', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_txt = 'Hello! I love that you are so poor.'\n",
    "tokens_sample = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens_sample)\n",
    "print(token_ids) \n",
    "print(len(token_ids)) \n",
    "print(tokens_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28b7dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP] 102\n",
      "[CLS] 101\n",
      "[PAD] 0\n",
      "[UNK] 100\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sep_token, tokenizer.sep_token_id) # end of sentence marker\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of sentence token \n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id) # token for padding\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id) # token for unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05cf026",
   "metadata": {},
   "source": [
    "Then, we do embedding on the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1d950958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "tensor([[ 101, 8667,  106,  146, 1567, 1115, 1128, 1132, 1177, 2869,  119,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=32,\n",
    "  truncation = True, # truncate examples to max length \n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  padding = \"max_length\", \n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(encoding.keys()) # dict_keys(['input_ids', 'attention_mask'])\n",
    "print(encoding.input_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "67c5aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be021caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "51eb8c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7328, 7), (1571, 7), (1570, 7))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "  df,\n",
    "  test_size=0.3, \n",
    "  random_state = RANDOM_SEED\n",
    ")\n",
    "\n",
    "df_test, df_val = train_test_split(\n",
    "  df_test,\n",
    "  test_size=0.5, \n",
    "  random_state = RANDOM_SEED\n",
    ")\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ecc332c",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c964524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "  def __init__(self,dataframe, tokenizer, max_len):\n",
    "    self.x=dataframe.iloc[:,0:-3]\n",
    "    self.y=dataframe.iloc[:,-2]\n",
    "\n",
    "    self.x = np.array(self.x)\n",
    "    self.y = np.array(self.y)\n",
    "    \n",
    "    # self.x_train=torch.from_numpy(self.x)\n",
    "    # self.y_train=torch.from_numpy(self.y)\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "    input = str(self.x[idx])\n",
    "    labels = self.y[idx]\n",
    "\n",
    "    encoding = tokenizer.encode_plus(\n",
    "      input,\n",
    "      max_length= self.max_len,\n",
    "      truncation = True, # truncate examples to max length \n",
    "      add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "      return_token_type_ids=False,\n",
    "      padding = \"max_length\", \n",
    "      return_attention_mask=True,\n",
    "      return_tensors='pt')  # Return PyTorch tensor\n",
    "\n",
    "    return {\n",
    "      'text': input,\n",
    "      'input_ids': encoding['input_ids'].flatten(),\n",
    "      'attention_mask': encoding['attention_mask'].flatten(),\n",
    "      'targets': torch.tensor(labels, dtype=torch.long)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8f120a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': \"['1948' '@@15370913' 'immigrant' 'ca']\", 'input_ids': tensor([  101,   164,   112,  3027,   112,   112,   137,   137, 17777, 20829,\n",
      "         1580, 17668,   112,   112, 12338,   112,   112, 11019,   112,   166,\n",
      "          102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'targets': tensor(0)}\n"
     ]
    }
   ],
   "source": [
    "sample_dataset = MyDataset(df_train, tokenizer, 160)\n",
    "print(sample_dataset[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b44bbab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9fb4aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = MyDataset(\n",
    "    dataframe = df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=2\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8de0b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 160 # can change this \n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c1b0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 160])\n",
      "torch.Size([16, 160])\n",
      "torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n",
    "# print(data)\n",
    "\n",
    "print(data['input_ids'].shape)\n",
    "print(data['attention_mask'].shape)\n",
    "print(data['targets'].shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55fb619",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d1c150c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A\n",
      "\u001b[A"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e848d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a710e41c65c73894501f04823ff7eb1542720e9cc65ccbbb2db205b59bb67a27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('gen': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
