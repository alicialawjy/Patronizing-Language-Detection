{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "d5a478b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer, RobertaConfig, RobertaModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from dont_patronize_me import DontPatronizeMe\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "209eb94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map of label to numerical label:\n",
      "{'Unbalanced_power_relations': 0, 'Shallow_solution': 1, 'Presupposition': 2, 'Authority_voice': 3, 'Metaphors': 4, 'Compassion': 5, 'The_poorer_the_merrier': 6}\n"
     ]
    }
   ],
   "source": [
    "dpm = DontPatronizeMe('.', '.')\n",
    "dpm = DontPatronizeMe('.', 'dontpatronizeme_pcl.tsv')\n",
    "\n",
    "dpm.load_task1()\n",
    "\n",
    "dpm2 = DontPatronizeMe('.', '.')\n",
    "dpm2 = DontPatronizeMe('.', 'dontpatronizeme_categories.tsv')\n",
    "\n",
    "dpm2.load_task2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "0043f7a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dpm.train_task1_df\n",
    "df_cate = dpm2.train_task2_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f739909b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>par_id</th>\n",
       "      <th>art_id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>country</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>orig_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@@24942188</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>ph</td>\n",
       "      <td>We 're living in times of absolute insanity , ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@@21968160</td>\n",
       "      <td>migrant</td>\n",
       "      <td>gh</td>\n",
       "      <td>In Libya today , there are countless number of...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@@16584954</td>\n",
       "      <td>immigrant</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"White House press secretary Sean Spicer said ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@@7811231</td>\n",
       "      <td>disabled</td>\n",
       "      <td>nz</td>\n",
       "      <td>Council customers only signs would be displaye...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@@1494111</td>\n",
       "      <td>refugee</td>\n",
       "      <td>ca</td>\n",
       "      <td>\"\"\" Just like we received migrants fleeing El ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10464</th>\n",
       "      <td>10465</td>\n",
       "      <td>@@14297363</td>\n",
       "      <td>women</td>\n",
       "      <td>lk</td>\n",
       "      <td>\"Sri Lankan norms and culture inhibit women fr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10465</th>\n",
       "      <td>10466</td>\n",
       "      <td>@@70091353</td>\n",
       "      <td>vulnerable</td>\n",
       "      <td>ph</td>\n",
       "      <td>He added that the AFP will continue to bank on...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10466</th>\n",
       "      <td>10467</td>\n",
       "      <td>@@20282330</td>\n",
       "      <td>in-need</td>\n",
       "      <td>ng</td>\n",
       "      <td>\"\"\" She has one huge platform , and informatio...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10467</th>\n",
       "      <td>10468</td>\n",
       "      <td>@@16753236</td>\n",
       "      <td>hopeless</td>\n",
       "      <td>in</td>\n",
       "      <td>\"\"\" Anja Ringgren Loven I ca n't find a word t...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10468</th>\n",
       "      <td>10469</td>\n",
       "      <td>@@16779383</td>\n",
       "      <td>homeless</td>\n",
       "      <td>ie</td>\n",
       "      <td>\"\"\" Guinness World Record of 540lbs of 7-layer...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10469 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      par_id      art_id     keyword country  \\\n",
       "0          1  @@24942188    hopeless      ph   \n",
       "1          2  @@21968160     migrant      gh   \n",
       "2          3  @@16584954   immigrant      ie   \n",
       "3          4   @@7811231    disabled      nz   \n",
       "4          5   @@1494111     refugee      ca   \n",
       "...      ...         ...         ...     ...   \n",
       "10464  10465  @@14297363       women      lk   \n",
       "10465  10466  @@70091353  vulnerable      ph   \n",
       "10466  10467  @@20282330     in-need      ng   \n",
       "10467  10468  @@16753236    hopeless      in   \n",
       "10468  10469  @@16779383    homeless      ie   \n",
       "\n",
       "                                                    text  label orig_label  \n",
       "0      We 're living in times of absolute insanity , ...      0          0  \n",
       "1      In Libya today , there are countless number of...      0          0  \n",
       "2      \"White House press secretary Sean Spicer said ...      0          0  \n",
       "3      Council customers only signs would be displaye...      0          0  \n",
       "4      \"\"\" Just like we received migrants fleeing El ...      0          0  \n",
       "...                                                  ...    ...        ...  \n",
       "10464  \"Sri Lankan norms and culture inhibit women fr...      0          1  \n",
       "10465  He added that the AFP will continue to bank on...      0          0  \n",
       "10466  \"\"\" She has one huge platform , and informatio...      1          3  \n",
       "10467  \"\"\" Anja Ringgren Loven I ca n't find a word t...      1          4  \n",
       "10468  \"\"\" Guinness World Record of 540lbs of 7-layer...      1          3  \n",
       "\n",
       "[10469 rows x 7 columns]"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ad2305",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "7cd9bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=[]\n",
    "new= df['text'].str.split()\n",
    "new=new.values.tolist()\n",
    "corpus=[word for i in new for word in i]\n",
    "\n",
    "from collections import defaultdict\n",
    "my_dict=defaultdict(int)\n",
    "for word in corpus:\n",
    "    my_dict[word]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "dcde2fea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "keys = list(my_dict.keys())\n",
    "# get values in the same order as keys, and parse percentage values\n",
    "vals = [int(my_dict[k]) for k in keys]\n",
    "sns.barplot(x=keys, y=vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "830deec6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['We',\n",
       " \"'re\",\n",
       " 'living',\n",
       " 'in',\n",
       " 'times',\n",
       " 'of',\n",
       " 'absolute',\n",
       " 'insanity',\n",
       " ',',\n",
       " 'as',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'pretty',\n",
       " 'sure',\n",
       " 'most',\n",
       " 'people',\n",
       " 'are',\n",
       " 'aware',\n",
       " '.',\n",
       " 'For',\n",
       " 'a',\n",
       " 'while',\n",
       " 'waking',\n",
       " 'up',\n",
       " 'every',\n",
       " 'day',\n",
       " 'to',\n",
       " 'check',\n",
       " 'the',\n",
       " 'news',\n",
       " 'seemed',\n",
       " 'carry',\n",
       " 'with',\n",
       " 'it',\n",
       " 'same',\n",
       " 'feeling',\n",
       " 'panic',\n",
       " 'and',\n",
       " 'dread',\n",
       " 'that',\n",
       " 'action',\n",
       " 'heroes',\n",
       " 'probably',\n",
       " 'face',\n",
       " 'when',\n",
       " 'they',\n",
       " 'trying',\n",
       " 'decide',\n",
       " 'whether',\n",
       " 'cut',\n",
       " 'blue',\n",
       " 'or',\n",
       " 'green',\n",
       " 'wire',\n",
       " 'on',\n",
       " 'ticking',\n",
       " 'bomb',\n",
       " '--',\n",
       " 'except',\n",
       " \"'s\",\n",
       " 'instructions',\n",
       " 'long',\n",
       " 'ago',\n",
       " 'burned',\n",
       " 'fire',\n",
       " 'imminent',\n",
       " 'catastrophe',\n",
       " 'seems',\n",
       " 'likeliest',\n",
       " 'outcome',\n",
       " 'It',\n",
       " 'hard',\n",
       " 'stay',\n",
       " 'on-edge',\n",
       " 'for',\n",
       " 'though',\n",
       " 'so',\n",
       " 'natural',\n",
       " 'become',\n",
       " 'inured',\n",
       " 'this',\n",
       " 'constant',\n",
       " 'chaos',\n",
       " 'slump',\n",
       " 'into',\n",
       " 'malaise',\n",
       " 'hopelessness',\n",
       " 'pessimism',\n",
       " 'In',\n",
       " 'Libya',\n",
       " 'today',\n",
       " 'there',\n",
       " 'countless',\n",
       " 'number',\n",
       " 'Ghanaian',\n",
       " 'Nigerian',\n",
       " 'immigrants',\n",
       " 'These',\n",
       " 'two',\n",
       " 'countries',\n",
       " 'key',\n",
       " 'macroeconomic',\n",
       " 'challenges',\n",
       " 'including',\n",
       " 'unemployment',\n",
       " 'Let',\n",
       " 'tackle',\n",
       " 'issue',\n",
       " 'from',\n",
       " 'root',\n",
       " 'not',\n",
       " 'fruit',\n",
       " 'Thank',\n",
       " 'you',\n",
       " '\"White',\n",
       " 'House',\n",
       " 'press',\n",
       " 'secretary',\n",
       " 'Sean',\n",
       " 'Spicer',\n",
       " 'said',\n",
       " 'focus',\n",
       " 'immigration',\n",
       " 'actions',\n",
       " 'would',\n",
       " 'be',\n",
       " 'illegal',\n",
       " '\"\"',\n",
       " 'who',\n",
       " 'have',\n",
       " 'also',\n",
       " 'otherwise',\n",
       " 'violated',\n",
       " 'our',\n",
       " 'laws',\n",
       " '.\"',\n",
       " 'Council',\n",
       " 'customers',\n",
       " 'only',\n",
       " 'signs',\n",
       " 'displayed',\n",
       " 'Two',\n",
       " 'spaces',\n",
       " 'reserved',\n",
       " 'disabled',\n",
       " 'persons',\n",
       " 'five',\n",
       " 'P30',\n",
       " 'eight',\n",
       " 'P60',\n",
       " 'ones',\n",
       " '\"\"\"',\n",
       " 'Just',\n",
       " 'like',\n",
       " 'we',\n",
       " 'received',\n",
       " 'migrants',\n",
       " 'fleeing',\n",
       " 'El',\n",
       " 'Salvador',\n",
       " 'Guatemala',\n",
       " \"'\",\n",
       " '80s',\n",
       " '90s',\n",
       " 'just',\n",
       " \"'ve\",\n",
       " 'seen',\n",
       " 'thousands',\n",
       " 'economic',\n",
       " 'refugees',\n",
       " 'started',\n",
       " 'seeing',\n",
       " 'Mexicans',\n",
       " 'violence',\n",
       " 'Ju',\n",
       " '?',\n",
       " 'rez',\n",
       " 'other',\n",
       " 'parts',\n",
       " 'Mexico',\n",
       " 'says',\n",
       " 'shelter',\n",
       " 'director',\n",
       " 'Ruben',\n",
       " 'Garcia',\n",
       " 'To',\n",
       " 'bring',\n",
       " 'down',\n",
       " 'high',\n",
       " 'blood',\n",
       " 'sugar',\n",
       " 'levels',\n",
       " 'insulin',\n",
       " 'needs',\n",
       " 'taken',\n",
       " 'If',\n",
       " 'type',\n",
       " 'requires',\n",
       " 'during',\n",
       " 'meal',\n",
       " 'time',\n",
       " 'will',\n",
       " 'take',\n",
       " 'correct',\n",
       " 'doses',\n",
       " 'order',\n",
       " 'lower',\n",
       " 'your',\n",
       " 'glucose',\n",
       " 'Decision',\n",
       " 'inject',\n",
       " 'how',\n",
       " 'many',\n",
       " 'help',\n",
       " 'health',\n",
       " 'care',\n",
       " 'professional',\n",
       " 'The',\n",
       " 'European',\n",
       " 'Union',\n",
       " 'is',\n",
       " 'making',\n",
       " 'an',\n",
       " 'historic',\n",
       " 'mistake',\n",
       " 'its',\n",
       " 'haste',\n",
       " 'conclude',\n",
       " 'refugee',\n",
       " 'deal',\n",
       " 'Turkey',\n",
       " 'overlooking',\n",
       " 'human',\n",
       " 'rights',\n",
       " 'violations',\n",
       " 'risk',\n",
       " 'plunging',\n",
       " 'bloc',\n",
       " 'largest',\n",
       " 'membership',\n",
       " 'candidate',\n",
       " 'civil',\n",
       " 'war',\n",
       " 'Selahattin',\n",
       " 'Demirtas',\n",
       " 'leader',\n",
       " 'nation',\n",
       " 'prominent',\n",
       " 'pro-Kurdish',\n",
       " 'party',\n",
       " 'They',\n",
       " 'either',\n",
       " 'hopeless',\n",
       " 'being',\n",
       " 'beaten',\n",
       " 'by',\n",
       " '10-year-old',\n",
       " 'if',\n",
       " 'beat',\n",
       " 'him',\n",
       " 'big',\n",
       " 'bully',\n",
       " 'That',\n",
       " 'joke',\n",
       " 'reality',\n",
       " 'he',\n",
       " 'got',\n",
       " 'great',\n",
       " 'game',\n",
       " 'NUEVA',\n",
       " 'ERA',\n",
       " 'Ilocos',\n",
       " 'Norte',\n",
       " '-',\n",
       " 'No',\n",
       " 'family',\n",
       " 'shall',\n",
       " 'homeless',\n",
       " 'under',\n",
       " 'watch',\n",
       " 'municipal',\n",
       " 'government',\n",
       " 'here',\n",
       " 'town',\n",
       " 'Mayor',\n",
       " 'Aldrin',\n",
       " 'Garvida',\n",
       " 'His',\n",
       " 'spokesman',\n",
       " 'Kremlin',\n",
       " 'needed',\n",
       " 'more',\n",
       " 'information',\n",
       " 'about',\n",
       " 'rebels',\n",
       " 'decision',\n",
       " 'He',\n",
       " 'rebel',\n",
       " 'statement',\n",
       " 'came',\n",
       " 'after',\n",
       " 'Western-backed',\n",
       " 'Kiev',\n",
       " 'had',\n",
       " 'declared',\n",
       " 'military',\n",
       " 'operation',\n",
       " 'implying',\n",
       " 'Ukraine',\n",
       " 'was',\n",
       " 'blame',\n",
       " 'refusal',\n",
       " 'heed',\n",
       " 'Putin',\n",
       " 'A',\n",
       " 'federal',\n",
       " 'appeals',\n",
       " 'court',\n",
       " 'Tuesday',\n",
       " 'cleared',\n",
       " 'way',\n",
       " '17-year-old',\n",
       " 'immigrant',\n",
       " 'held',\n",
       " 'custody',\n",
       " 'Texas',\n",
       " 'obtain',\n",
       " 'abortion',\n",
       " 'full',\n",
       " 'US',\n",
       " 'Court',\n",
       " 'Appeals',\n",
       " 'District',\n",
       " 'Columbia',\n",
       " 'Circuit',\n",
       " 'ruled',\n",
       " '6-3',\n",
       " 'favor',\n",
       " 'teen',\n",
       " 'overturned',\n",
       " 'ruling',\n",
       " 'three-judge',\n",
       " 'panel',\n",
       " 'at',\n",
       " 'least',\n",
       " 'temporarily',\n",
       " 'blocked',\n",
       " 'her',\n",
       " 'getting',\n",
       " 'Trump',\n",
       " 'administration',\n",
       " 'could',\n",
       " 'still',\n",
       " 'appeal',\n",
       " 'Supreme',\n",
       " 'find',\n",
       " 'ourselves',\n",
       " 'situation',\n",
       " 'because',\n",
       " 'longer',\n",
       " 'local',\n",
       " 'factors',\n",
       " 'requiring',\n",
       " 'nursing',\n",
       " 'their',\n",
       " 'home',\n",
       " 'increasing',\n",
       " 'coupled',\n",
       " 'increased',\n",
       " 'demands',\n",
       " 'council-funded',\n",
       " 'vulnerable',\n",
       " 'within',\n",
       " 'own',\n",
       " 'homes',\n",
       " 'Ghostbusters',\n",
       " 'resurrection',\n",
       " '1984',\n",
       " 'hit',\n",
       " 'film',\n",
       " 'but',\n",
       " 'leads',\n",
       " 'women',\n",
       " 'men',\n",
       " 'Government',\n",
       " 'commitment',\n",
       " 'protect',\n",
       " 'groups',\n",
       " 'Najib',\n",
       " '2017',\n",
       " 'Budget',\n",
       " 'proposed',\n",
       " 'financial',\n",
       " 'assistance',\n",
       " 'poor',\n",
       " 'families',\n",
       " 'General',\n",
       " 'Assistance',\n",
       " 'RM300',\n",
       " 'per',\n",
       " 'month',\n",
       " 'Children',\n",
       " 'RM450',\n",
       " 'When',\n",
       " 'my',\n",
       " 'dad',\n",
       " 'died',\n",
       " 'unexpectedly',\n",
       " '2014',\n",
       " 'putting',\n",
       " 'finishing',\n",
       " 'touches',\n",
       " 'his',\n",
       " 'forthcoming',\n",
       " 'book',\n",
       " 'Climate',\n",
       " 'Change',\n",
       " 'Health',\n",
       " 'Nations',\n",
       " 'examines',\n",
       " 'historical',\n",
       " 'records',\n",
       " 'finds',\n",
       " 'very',\n",
       " 'sensitive',\n",
       " 'changes',\n",
       " 'climate',\n",
       " 'And',\n",
       " 'drawing',\n",
       " 'history',\n",
       " 'warns',\n",
       " 'impacts',\n",
       " 'climatic',\n",
       " 'change',\n",
       " 'such',\n",
       " 'environmental',\n",
       " 'disaster',\n",
       " 'temperatures',\n",
       " 'food',\n",
       " 'insecurity',\n",
       " 'amplify',\n",
       " 'infectious',\n",
       " 'diseases',\n",
       " 'under-nutrition',\n",
       " 'heat-related',\n",
       " 'deaths',\n",
       " 'especially',\n",
       " 'among',\n",
       " 'populations',\n",
       " 'children',\n",
       " 'forcibly',\n",
       " 'displaced',\n",
       " 'Apart',\n",
       " 'Pakistan',\n",
       " 'hosts',\n",
       " 'England',\n",
       " 'Bangladesh',\n",
       " 'cricket',\n",
       " 'team',\n",
       " 'participate',\n",
       " 'tournament',\n",
       " '<h>',\n",
       " 'Shahid',\n",
       " 'Afridi',\n",
       " 'bags',\n",
       " '11',\n",
       " 'Man',\n",
       " 'Match',\n",
       " 'awards',\n",
       " 'T20Is',\n",
       " '(',\n",
       " 'Most',\n",
       " 'any',\n",
       " 'player',\n",
       " ')',\n",
       " 'Ab',\n",
       " 'De',\n",
       " 'villiers',\n",
       " 'bagged',\n",
       " 'pair',\n",
       " 'first',\n",
       " 'Test',\n",
       " 'career',\n",
       " 'Super',\n",
       " 'Gossip',\n",
       " 'those',\n",
       " 'few',\n",
       " 'seconds',\n",
       " 'humanity',\n",
       " 'free',\n",
       " 'shackles',\n",
       " 'one',\n",
       " 'glimpses',\n",
       " 'hope',\n",
       " 'amid',\n",
       " 'Peace',\n",
       " 'returned',\n",
       " 'Assam',\n",
       " 'both',\n",
       " 'Hindus',\n",
       " 'Muslims',\n",
       " 'agreed',\n",
       " 'accept',\n",
       " '1971',\n",
       " 'cut-off',\n",
       " 'year',\n",
       " 'citizenship',\n",
       " 'Today',\n",
       " 'wants',\n",
       " 'push',\n",
       " 'state',\n",
       " 'towards',\n",
       " 'unrest',\n",
       " 'convention',\n",
       " 'organized',\n",
       " 'Krishak',\n",
       " 'Mukti',\n",
       " 'Sangram',\n",
       " 'Samiti',\n",
       " 'urged',\n",
       " 'arrived',\n",
       " 'before',\n",
       " 'launch',\n",
       " 'united',\n",
       " 'movement',\n",
       " 'scuttle',\n",
       " 'Centre',\n",
       " 'plan',\n",
       " 'regularize',\n",
       " 'entry',\n",
       " 'Hindu',\n",
       " 'She',\n",
       " 'since',\n",
       " 'total',\n",
       " '8000',\n",
       " 'intellectually',\n",
       " 'Ghana',\n",
       " 'been',\n",
       " 'offered',\n",
       " 'special',\n",
       " 'training',\n",
       " 'enhance',\n",
       " 'integration',\n",
       " 'society',\n",
       " '\"In',\n",
       " 'short',\n",
       " 'reasons',\n",
       " 'behind',\n",
       " 'look',\n",
       " 'reasonable',\n",
       " 'problem',\n",
       " 'however',\n",
       " 'has',\n",
       " 'implementation',\n",
       " 'side',\n",
       " 'much',\n",
       " 'talk',\n",
       " 'success',\n",
       " 'grounds',\n",
       " 'all',\n",
       " 'left',\n",
       " 'voluntarily',\n",
       " 'origin',\n",
       " 'others',\n",
       " 'repatriated',\n",
       " 'leaving',\n",
       " 'country',\n",
       " 'given',\n",
       " 'circumstances',\n",
       " 'optional',\n",
       " 'Sisi',\n",
       " 'lawyer',\n",
       " 'Grisel',\n",
       " 'Ybarra',\n",
       " 'Cuban',\n",
       " 'community',\n",
       " 'edge',\n",
       " 'ongoing',\n",
       " 'negotiations',\n",
       " 'between',\n",
       " 'Washington',\n",
       " 'Havana',\n",
       " 'uncertainty',\n",
       " 'what',\n",
       " 'renewed',\n",
       " 'relations',\n",
       " 'mean',\n",
       " 'Might',\n",
       " 'pain',\n",
       " 'Dr.',\n",
       " 'Francis',\n",
       " 'Javier',\n",
       " 'until',\n",
       " 'recently',\n",
       " 'headed',\n",
       " 'Pain',\n",
       " 'Management',\n",
       " 'Center',\n",
       " 'St.',\n",
       " 'Luke',\n",
       " 'Medical',\n",
       " 'looking',\n",
       " 'genetics',\n",
       " 'comparing',\n",
       " 'Filipinos',\n",
       " 'mixed',\n",
       " 'ancestries',\n",
       " 'see',\n",
       " 'differences',\n",
       " 'medicines',\n",
       " 'broken',\n",
       " 'body',\n",
       " 'highlights',\n",
       " 'different',\n",
       " 'communities',\n",
       " 'respond',\n",
       " 'homelessness',\n",
       " 'ways',\n",
       " 'Kneebone',\n",
       " 'regional',\n",
       " 'brands',\n",
       " 'far',\n",
       " 'lag',\n",
       " 'global',\n",
       " 'international',\n",
       " 'Chinese',\n",
       " 'handset',\n",
       " 'players',\n",
       " '4G',\n",
       " 'looked',\n",
       " 'failing',\n",
       " 'jump',\n",
       " 'generation',\n",
       " 'successfully',\n",
       " 'lose',\n",
       " 'place',\n",
       " 'incidents',\n",
       " 'deepened',\n",
       " 'public',\n",
       " 'doubts',\n",
       " 'Chancellor',\n",
       " 'Angela',\n",
       " 'Merkel',\n",
       " 'open',\n",
       " 'Germany',\n",
       " 'doors',\n",
       " 'whose',\n",
       " 'numbers',\n",
       " 'reached',\n",
       " '1.1',\n",
       " 'million',\n",
       " 'last',\n",
       " 'Some',\n",
       " 'say',\n",
       " 'review',\n",
       " 'legislation',\n",
       " 'sex',\n",
       " 'work',\n",
       " 'helps',\n",
       " 'involved',\n",
       " 'services',\n",
       " 'provided',\n",
       " 'these',\n",
       " 'think',\n",
       " 'know',\n",
       " 'led',\n",
       " 'me',\n",
       " 'detained',\n",
       " 'hours',\n",
       " 'opposed',\n",
       " 'another',\n",
       " 'Sudanese',\n",
       " 'person',\n",
       " '30',\n",
       " 'seventies',\n",
       " 'So',\n",
       " 'aspect',\n",
       " 'On',\n",
       " 'flip',\n",
       " 'went',\n",
       " 'Terminal',\n",
       " '4',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'background',\n",
       " 'feel',\n",
       " 'guess',\n",
       " 'point',\n",
       " 'actually',\n",
       " 'want',\n",
       " 'make',\n",
       " 'reflection',\n",
       " 'larger',\n",
       " 'trend',\n",
       " 'criminalise',\n",
       " 'black',\n",
       " 'Muslim',\n",
       " 'really',\n",
       " 'concerned',\n",
       " 'do',\n",
       " 'Somalis',\n",
       " 'African',\n",
       " 'descent',\n",
       " 'going',\n",
       " 'affected',\n",
       " 'treated',\n",
       " 'differently',\n",
       " 'frankly',\n",
       " 'This',\n",
       " '4th',\n",
       " 'July',\n",
       " 'falls',\n",
       " 'Monday',\n",
       " 'mere',\n",
       " '24',\n",
       " 'kick',\n",
       " 'back',\n",
       " 'and/or',\n",
       " 'holiday',\n",
       " 'grind',\n",
       " 'With',\n",
       " 'crunch',\n",
       " 'can',\n",
       " 'understand',\n",
       " 'might',\n",
       " 'seem',\n",
       " 'never',\n",
       " 'fear',\n",
       " ':',\n",
       " 'Lifehacker',\n",
       " '!',\n",
       " 'Here',\n",
       " 'some',\n",
       " 'tips',\n",
       " 'dive',\n",
       " 'headfirst',\n",
       " 'come',\n",
       " 'out',\n",
       " 'intact',\n",
       " 'Farooqui',\n",
       " 'were',\n",
       " 'demanding',\n",
       " 'fast-track',\n",
       " 'courts',\n",
       " 'rape',\n",
       " 'crimes',\n",
       " 'against',\n",
       " 'right',\n",
       " 'Why',\n",
       " 'pay',\n",
       " '50,000',\n",
       " 'riel',\n",
       " 'acknowledge',\n",
       " 'Cambodian',\n",
       " 'citizen',\n",
       " 'early',\n",
       " 'thing',\n",
       " 'Mr.',\n",
       " 'Yang',\n",
       " 'Ros',\n",
       " 'asked',\n",
       " 'Nearly',\n",
       " '110',\n",
       " 'made',\n",
       " 'blaze',\n",
       " 'housed',\n",
       " 'hotels',\n",
       " 'west',\n",
       " 'London',\n",
       " 'Churches',\n",
       " 'centres',\n",
       " 'providing',\n",
       " 'meals',\n",
       " 'support',\n",
       " 'donations',\n",
       " 'clothing',\n",
       " 'toys',\n",
       " 'household',\n",
       " 'supplies',\n",
       " 'flooding',\n",
       " '\"Cardinal',\n",
       " 'Tagle',\n",
       " 'visited',\n",
       " 'settlements',\n",
       " 'part',\n",
       " 'role',\n",
       " 'Caritas',\n",
       " 'president',\n",
       " 'asks',\n",
       " 'express',\n",
       " 'reservations',\n",
       " 'receiving',\n",
       " 'Have',\n",
       " 'ever',\n",
       " 'talked',\n",
       " 'real',\n",
       " 'heard',\n",
       " 'stories',\n",
       " 'Arshad',\n",
       " 'besides',\n",
       " 'learning',\n",
       " 'new',\n",
       " 'aspects',\n",
       " 'sports',\n",
       " 'leadership',\n",
       " 'learnt',\n",
       " 'fast-developing',\n",
       " 'nations',\n",
       " 'using',\n",
       " 'tool',\n",
       " 'development',\n",
       " 'effort',\n",
       " 'underprivileged',\n",
       " 'stage',\n",
       " 'Fast',\n",
       " 'employee',\n",
       " 'fed',\n",
       " 'man',\n",
       " 'becomes',\n",
       " 'internet',\n",
       " 'sensation',\n",
       " '\"The',\n",
       " '28',\n",
       " 'leaders',\n",
       " 'ask',\n",
       " 'Davutoglu',\n",
       " 'large-scale',\n",
       " 'deportations',\n",
       " 'Greece',\n",
       " 'main',\n",
       " 'Europe',\n",
       " 'implement',\n",
       " 'November',\n",
       " 'slow',\n",
       " 'flow',\n",
       " 'foundation',\n",
       " 'willing',\n",
       " 'raise',\n",
       " 'fund',\n",
       " 'victims',\n",
       " 'click',\n",
       " 'link',\n",
       " 'below',\n",
       " 'donation',\n",
       " 'broke',\n",
       " 'labor',\n",
       " 'camp',\n",
       " 'Al',\n",
       " 'Sailiya',\n",
       " '8th',\n",
       " 'May',\n",
       " 'approximately',\n",
       " '300',\n",
       " 'Sri',\n",
       " 'Lankans',\n",
       " 'employed',\n",
       " 'Janitorial',\n",
       " 'Service',\n",
       " 'Company',\n",
       " 'Qatar',\n",
       " 'temporary',\n",
       " 'lodging',\n",
       " 'Expatriates',\n",
       " 'report',\n",
       " 'arrested',\n",
       " 'authorities',\n",
       " 'Rizvi',\n",
       " 'There',\n",
       " 'no',\n",
       " 'joy',\n",
       " 'Eid',\n",
       " 'citizens',\n",
       " 'becoming',\n",
       " 'FLOODTwo',\n",
       " 'weeks',\n",
       " 'farms',\n",
       " 'submerged',\n",
       " 'water',\n",
       " 'rendering',\n",
       " 'Garissa',\n",
       " 'Tana',\n",
       " 'River',\n",
       " '800,000',\n",
       " 'undocumented',\n",
       " 'expected',\n",
       " 'benefit',\n",
       " 'policy',\n",
       " 'them',\n",
       " 'receive',\n",
       " 'deferred',\n",
       " 'deportation',\n",
       " 'Undocumented',\n",
       " 'U.S.',\n",
       " 'age',\n",
       " '16',\n",
       " 'lived',\n",
       " 'than',\n",
       " 'years',\n",
       " 'apply',\n",
       " 'relief',\n",
       " 'once',\n",
       " 'Justin',\n",
       " 'Bour',\n",
       " 'off',\n",
       " 'ninth',\n",
       " '22nd',\n",
       " 'homer',\n",
       " 'six-week',\n",
       " 'stint',\n",
       " 'list',\n",
       " 'Ramos',\n",
       " 'then',\n",
       " 'gave',\n",
       " 'four',\n",
       " 'singles',\n",
       " 'two-out',\n",
       " 'RBI',\n",
       " 'hits',\n",
       " 'pinch-hitter',\n",
       " 'A.J.',\n",
       " 'Ellis',\n",
       " 'Ichiro',\n",
       " 'Suzuki',\n",
       " 'Vanessa',\n",
       " 'feelings',\n",
       " 'final',\n",
       " 'days',\n",
       " 'which',\n",
       " 'cry',\n",
       " 'yielded',\n",
       " 'response',\n",
       " 'resident',\n",
       " 'magistrate',\n",
       " 'placed',\n",
       " 'State',\n",
       " 'Her',\n",
       " 'letter',\n",
       " 'conveyed',\n",
       " 'official',\n",
       " 'investigator',\n",
       " 'Office',\n",
       " 'Advocate',\n",
       " 'able',\n",
       " 'access',\n",
       " 'Gwauya',\n",
       " 'mother',\n",
       " 'Sithela',\n",
       " 'told',\n",
       " 'Thomson',\n",
       " 'Reuters',\n",
       " 'Foundation',\n",
       " 'Comrade',\n",
       " 'David',\n",
       " 'Kerigbo',\n",
       " 'Ugondo',\n",
       " 'born',\n",
       " 'Late',\n",
       " 'Pa',\n",
       " 'Akerigbo',\n",
       " 'Adikpo',\n",
       " 'wife',\n",
       " 'Mrs.',\n",
       " 'Pam',\n",
       " '25th',\n",
       " 'October',\n",
       " '1950',\n",
       " 'Achagh',\n",
       " 'Mbaduku',\n",
       " 'Vandeikya',\n",
       " 'Local',\n",
       " 'Benue',\n",
       " '67th',\n",
       " 'birthday',\n",
       " 'well',\n",
       " 'secured',\n",
       " 'Nigeria',\n",
       " 'birth',\n",
       " 'nurtured',\n",
       " '27',\n",
       " 'meritorious',\n",
       " 'service',\n",
       " 'industry',\n",
       " 'unionism',\n",
       " 'tragic',\n",
       " 'unsecured',\n",
       " 'day-light',\n",
       " 'gunshots',\n",
       " 'Sunday',\n",
       " 'September',\n",
       " '10',\n",
       " 'criminal',\n",
       " 'armed',\n",
       " 'robbers',\n",
       " 'attacked',\n",
       " 'around',\n",
       " 'Birnin',\n",
       " 'Gwari',\n",
       " 'Kaduna',\n",
       " 'God',\n",
       " 'grant',\n",
       " ...]"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "40741202",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-cased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed6fd19",
   "metadata": {},
   "source": [
    "### Sample text to visualise tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "068ce4f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8667, 106, 146, 1567, 1115, 1128, 1132, 1177, 2869, 119]\n",
      "10\n",
      "['Hello', '!', 'I', 'love', 'that', 'you', 'are', 'so', 'poor', '.']\n"
     ]
    }
   ],
   "source": [
    "sample_txt = 'Hello! I love that you are so poor.'\n",
    "tokens_sample = tokenizer.tokenize(sample_txt)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens_sample)\n",
    "print(token_ids) \n",
    "print(len(token_ids)) \n",
    "print(tokens_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "28b7dc88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SEP] 102\n",
      "[CLS] 101\n",
      "[PAD] 0\n",
      "[UNK] 100\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.sep_token, tokenizer.sep_token_id) # end of sentence marker\n",
    "print(tokenizer.cls_token, tokenizer.cls_token_id) # start of sentence token \n",
    "print(tokenizer.pad_token, tokenizer.pad_token_id) # token for padding\n",
    "print(tokenizer.unk_token, tokenizer.unk_token_id) # token for unknown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05cf026",
   "metadata": {},
   "source": [
    "Then, we do embedding on the tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "1d950958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask'])\n",
      "tensor([[ 101, 8667,  106,  146, 1567, 1115, 1128, 1132, 1177, 2869,  119,  102,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "encoding = tokenizer.encode_plus(\n",
    "  sample_txt,\n",
    "  max_length=32,\n",
    "  truncation = True, # truncate examples to max length \n",
    "  add_special_tokens=True, # Add '[CLS]' and '[SEP]'\n",
    "  return_token_type_ids=False,\n",
    "  padding = \"max_length\", \n",
    "  return_attention_mask=True,\n",
    "  return_tensors='pt',  # Return PyTorch tensors\n",
    ")\n",
    "\n",
    "print(encoding.keys()) # dict_keys(['input_ids', 'attention_mask'])\n",
    "print(encoding.input_ids) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "67c5aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])\n"
     ]
    }
   ],
   "source": [
    "print(encoding['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be021caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentimentClassifier(nn.Module):\n",
    "  def __init__(self, n_classes):\n",
    "    super(SentimentClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained(PRE_TRAINED_MODEL_NAME)\n",
    "    self.drop = nn.Dropout(p=0.3)\n",
    "    self.out = nn.Linear(self.bert.config.hidden_size, n_classes)\n",
    "  def forward(self, input_ids, attention_mask):\n",
    "    _, pooled_output = self.bert(\n",
    "      input_ids=input_ids,\n",
    "      attention_mask=attention_mask\n",
    "    )\n",
    "    output = self.drop(pooled_output)\n",
    "    return self.out(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "51eb8c76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((7328, 7), (1571, 7), (1570, 7))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train, df_test = train_test_split(\n",
    "  df,\n",
    "  test_size=0.3, \n",
    "  random_state = RANDOM_SEED\n",
    ")\n",
    "\n",
    "df_test, df_val = train_test_split(\n",
    "  df_test,\n",
    "  test_size=0.5, \n",
    "  random_state = RANDOM_SEED\n",
    ")\n",
    "\n",
    "df_train.shape, df_val.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1884274",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ecc332c",
   "metadata": {},
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c964524",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "\n",
    "  def __init__(self,dataframe, tokenizer, max_len):\n",
    "    self.x=dataframe.iloc[:,0:-3]\n",
    "    self.y=dataframe.iloc[:,-2]\n",
    "\n",
    "    self.x_train=torch.tensor(self.x,dtype=torch.float32)\n",
    "    self.y_train=torch.tensor(self.y,dtype=torch.float32)\n",
    "\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_len = max_len\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.y_train)\n",
    "  \n",
    "  def __getitem__(self,idx):\n",
    "      encoding = self.tokenizer.encode_plus(\n",
    "        self.x,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt')\n",
    "\n",
    "      return {\n",
    "        'text': self.x_train,\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'targets': torch.tensor(self.y_train, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "9fb4aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(df, tokenizer, max_len, batch_size):\n",
    "  ds = MyDataset(\n",
    "    dataframe = df,\n",
    "    tokenizer=tokenizer,\n",
    "    max_len=max_len\n",
    "  )\n",
    "\n",
    "  return DataLoader(\n",
    "    ds,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=1\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "8de0b497",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "MAX_LEN = 160 # can change this \n",
    "train_data_loader = create_data_loader(df_train, tokenizer, MAX_LEN, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1a0a5b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_DataLoader__initialized', '_DataLoader__multiprocessing_context', '_IterableDataset_len_called', '__annotations__', '__class__', '__class_getitem__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__module__', '__ne__', '__new__', '__orig_bases__', '__parameters__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_auto_collation', '_dataset_kind', '_get_iterator', '_index_sampler', '_is_protocol', '_iterator', 'batch_sampler', 'batch_size', 'check_worker_number_rationality', 'collate_fn', 'dataset', 'drop_last', 'generator', 'multiprocessing_context', 'num_workers', 'persistent_workers', 'pin_memory', 'prefetch_factor', 'sampler', 'timeout', 'worker_init_fn']\n"
     ]
    }
   ],
   "source": [
    "print(dir(train_data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "1c1b0b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/weijiechua/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/weijiechua/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'MyDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/qk/lxtkcfz111s0q71dflzrr_zm0000gn/T/ipykernel_46752/1673200281.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    304\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_worker_number_rationality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 305\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_MultiProcessingDataLoaderIter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0;31m#     before it starts, and __del__ tries to join but will get:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m#     AssertionError: can only join a started process.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0mw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_index_queues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex_queue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/process.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    119\u001b[0m                \u001b[0;34m'daemonic processes are not allowed to have children'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0m_cleanup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_popen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# Avoid a refcycle if the target function holds an indirect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mDefaultContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseContext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/context.py\u001b[0m in \u001b[0;36m_Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0m_Popen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mpopen_spawn_posix\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32mclass\u001b[0m \u001b[0mForkServerProcess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBaseProcess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/popen_fork.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mduplicate_for_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/gen/lib/python3.9/multiprocessing/popen_spawn_posix.py\u001b[0m in \u001b[0;36m_launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentinel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparent_r\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mfds_to_close\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = next(iter(train_data_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79005de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OlidDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, tokenizer, dataframe):\n",
    "\n",
    "        self.tokenizer = tokenizer\n",
    "        x=dataframe.iloc[:,0:-3].values # not sure if we need values\n",
    "        y=dataframe.iloc[:,-2].values\n",
    "        \n",
    "        self.x_train=torch.tensor(x,dtype=torch.float32)\n",
    "        self.y_train=torch.tensor(y,dtype=torch.float32)\n",
    "\n",
    "    \n",
    "    def collate_fn(self, batch):\n",
    "        # batch is a dataframe, with input and label combined in a row\n",
    "        texts = []\n",
    "        labels = []\n",
    "\n",
    "        for b in batch: # let us figure out what this does \n",
    "            texts.append(batch.iloc[:,0:-3].values)\n",
    "            labels.append(batch.iloc[:,-2].values)\n",
    "\n",
    "        # The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
    "        # We also pad shorter sentences to a length of 128 tokens\n",
    "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "        labels = {}\n",
    "        encodings['label'] =  torch.tensor(labels)\n",
    "\n",
    "        return encodings\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "       # idx - index of the tweet\n",
    "       # labels\n",
    "        item = {'text': self.x_train[idx],\n",
    "                'label': self.labels[idx][0]}\n",
    "        return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1c150c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e3e848d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a710e41c65c73894501f04823ff7eb1542720e9cc65ccbbb2db205b59bb67a27"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('gen': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
